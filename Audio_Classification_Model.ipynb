{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdk0411/Audio_Classification_Model/blob/main/Audio_Classification_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0X3ic7z-OIU"
      },
      "source": [
        "1. 음성데이터 파일이름, Shape(XXX,), Sampling Rate(8000) 출력 코드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rk2s5xZIEF6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a042a619-2e71-4c3d-a01c-1f2fd9d852ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/My Drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID_H4dLXUxh0",
        "outputId": "765f9376-a247-4126-a857-d7ef7a75851b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Audio_Classification_Model'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-rqb10_735K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "Dataset_Path = '/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test'\n",
        "Data_dir = pathlib.Path(Dataset_Path)\n",
        "Aoudio_Path = list(Data_dir.glob('*.wav'))\n",
        "Auodio_list = []\n",
        "\n",
        "for wav_path in sorted(Aoudio_Path):\n",
        "    y, sr = librosa.load(wav_path, sr=None)\n",
        "    Auodio_list.append(f\"File Name: {wav_path.name} | Shape: {y.shape} | Sampling Rate: {sr}\")\n",
        "\n",
        "print('\\n'.join(Auodio_list))\n",
        "print(f\"Total Output Count: {len(Auodio_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jOKsWN6-gnn"
      },
      "source": [
        "2. MFCC 음성데이터 분석 코드\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMCgDLA_-fJ7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Dataset_Path = '/content/drive/MyDrive/Baby_Sound/Hungry/Music'\n",
        "Data_dir = pathlib.Path(Dataset_Path)\n",
        "MFCC_list = []\n",
        "\n",
        "all_wav_paths = list(Data_dir.glob('*.wav'))\n",
        "for wav_path in all_wav_paths:\n",
        "\n",
        "    y, sr = librosa.load(wav_path, sr=None)\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    MFCC_list.append(mfcc)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mfcc, x_axis='time')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"MFCC - {wav_path.name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "print(f\"Total MFCC_Auodio_Data Count : \", {len(MFCC_list)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgu895MMQ54C"
      },
      "source": [
        "3. Spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPTQbCuKQ5ho"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Dataset_Path = '/content/drive/MyDrive/Baby_Sound/Hungry/Music'\n",
        "Data_dir = pathlib.Path(Dataset_Path)\n",
        "\n",
        "# load wav files\n",
        "all_wav_paths = list(Data_dir.glob('*.wav'))\n",
        "for wav_path in all_wav_paths:\n",
        "    # read the wav file\n",
        "    y, sr = librosa.load(wav_path, sr=None)\n",
        "    # extract spectrogram\n",
        "    spec = librosa.stft(y)\n",
        "    spec_db = librosa.amplitude_to_db(abs(spec))\n",
        "    # plot spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(spec_db, x_axis='time', y_axis='log')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Spectrogram - {wav_path.name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud7uKtpcRHXc"
      },
      "source": [
        "4. Mel-Spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4FMZQgtRimL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Dataset_Path = '/content/drive/MyDrive/Baby_Sound/Hungry/Music'\n",
        "Data_dir = pathlib.Path(Dataset_Path)\n",
        "\n",
        "# load wav files\n",
        "all_wav_paths = list(Data_dir.glob('*.wav'))\n",
        "for wav_path in all_wav_paths:\n",
        "    # read the wav file\n",
        "    y, sr = librosa.load(wav_path, sr=None)\n",
        "    # extract mel-spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=40)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    # plot mel-spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel', sr=sr, fmax=sr/2)\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Mel-Spectrogram - {wav_path.name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLCpmIjBkctG"
      },
      "source": [
        "5. Spectrogram, Mel-Spectrogram, MFCC를 이용한 특징벡터화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocS2ec3OkT-3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "Dataset_Path = '/content/drive/MyDrive/Baby_Sound/Hungry/Music'\n",
        "Data_dir = pathlib.Path(Dataset_Path)\n",
        "Aoudio_Path = list(Data_dir.glob('*.wav'))\n",
        "\n",
        "Auodio_list = []\n",
        "# Audio_data 배열이 없으면 Audio_list배열에 문자열, 배열을 모두 담기때문에 오류가 발생함\n",
        "Audio_data = np.empty((0, 6 * sr))\n",
        "Sampling_Rate_Array = []\n",
        "\n",
        "for wav_path in sorted(Aoudio_Path):\n",
        "    y, sr = librosa.load(wav_path, sr=None)\n",
        "    Sampling_Rate_Array.append(sr)\n",
        "    Auodio_list.append(f\"File Name: {wav_path.name} | Shape: {y.shape} | Sampling Rate: {sr}\")\n",
        "\n",
        "print('\\n'.join(Auodio_list))\n",
        "print(f\"Total Output Count: {len(Audio_data)}\")\n",
        "\n",
        "# Hyper Parameter\n",
        "N_FFT = 1024\n",
        "HOP_SIZE = 512\n",
        "N_MELS = 128\n",
        "N_MFCC = 20\n",
        "\n",
        "def Compute_Aoudio(y, sr):\n",
        "    Spectrogram = librosa.stft(y, n_fft=N_FFT, hop_length=HOP_SIZE, center=False)\n",
        "    spect, phase = librosa.magphase(Spectrogram)\n",
        "    Mel_Spec = librosa.feature.melspectrogram(y, sr=sr, n_fft=N_FFT, hop_length=HOP_SIZE, n_mels=N_MELS, fmax=sr/2)\n",
        "    Mel_Spec_db = librosa.power_to_db(Mel_Spec, ref=np.max).T\n",
        "    MFCC_Spec = librosa.feature.mfcc(y, sr=sr, n_fft=N_FFT, hop_length=HOP_SIZE, n_mfcc=N_MFCC)\n",
        "    return spect.T, Mel_Spec_db, MFCC_Spec\n",
        "\n",
        "\n",
        "sr = Sampling_Rate_Array[0] # or any other value that applies to your audio files\n",
        "\n",
        "# Load audio data and compute features\n",
        "features = []\n",
        "for y in Audio_data:\n",
        "    spect, melspect, mfcc = Compute_Aoudio(y, sr)\n",
        "\n",
        "    # Concatenate features and append to list\n",
        "    feat = np.concatenate((spect, melspect, mfcc), axis=1)\n",
        "    features.append(feat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXa7y-iiNxkx"
      },
      "source": [
        "6. Data_set의 Label을 확인하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCyMmqsGzyxU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import librosa\n",
        "\n",
        "Dataset_Path = '/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data'\n",
        "\n",
        "file_list = [file_name for file_name in os.listdir(Dataset_Path) if file_name.endswith('.wav')]\n",
        "\n",
        "for file_name in file_list:\n",
        "    file_path = os.path.join(Dataset_Path, file_name)\n",
        "    y, sr = librosa.load(file_path, sr=None, duration=6.0)\n",
        "    label_size = len(file_name.split('-')[0])\n",
        "    print(f\"File Name: {file_name} | Label Size: {label_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji5n1HlK3dP5"
      },
      "source": [
        "7-1. CNN을 이용한 모델 학습 -> RMSprop, binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MFCC만 이용\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import regularizers\n",
        "\n",
        "# 기본 cuda GPU 사용, GPU 사용이 불가할 경우 CPU사용\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "def process_data(wav_path, csv_path):\n",
        "    X = []  # 빈 리스트로 초기화\n",
        "    labels = []  # 빈 리스트로 초기화\n",
        "    data_dir = pathlib.Path(wav_path)\n",
        "    all_wav_paths = list(data_dir.glob('*.wav'))\n",
        "\n",
        "    # CSV 파일 읽기\n",
        "    df = pd.read_csv(csv_path)\n",
        "    cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "    label = df[\"Label\"]\n",
        "\n",
        "    for wav_path_dir in all_wav_paths:\n",
        "        file_name = os.path.basename(wav_path_dir)\n",
        "        # 파일명과 일치하는 레이블 가져오기\n",
        "        index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "        label_value = label[index]\n",
        "\n",
        "        y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        # 높이 조절 -> 174로 조절함\n",
        "        if mfcc.shape[0] < 174:\n",
        "            pad_width = ((0, 174 - mfcc.shape[0]), (0, 0))\n",
        "            mfcc = np.pad(mfcc, pad_width, mode='constant')\n",
        "        elif mfcc.shape[0] > 174:\n",
        "            mfcc = mfcc[:174, :]\n",
        "        # 너비 조절 -> 188로 조절\n",
        "        if mfcc.shape[0] < 174:\n",
        "            pad_height = ((0, 174 - mfcc.shape[0]), (0, 0))\n",
        "            mfcc = np.pad(mfcc, pad_height, mode='constant')\n",
        "        elif mfcc.shape[0] > 174:\n",
        "            mfcc = mfcc[:174, :]\n",
        "        X.append(mfcc)  # 리스트에 mfcc 추가\n",
        "        labels.append(label_value)  # 가져온 레이블 추가\n",
        "\n",
        "    X = np.array(X)  # 리스트를 배열로 변환\n",
        "    labels = np.array(labels)\n",
        "    return X, labels\n",
        "\n",
        "# Train 데이터\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "# Test 데이터\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "X_train, y_train = process_data(train_data_dir, train_csv_path)\n",
        "X_test, y_test = process_data(test_data_dir, test_csv_path)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "# X_val = X_val[:, :, :174, :]\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(label_mapping) # 라벨 형태(str, int..) 출력\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# 학습에 필요한 요소 값 출력\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "print(\"num_classes:\", num_classes)\n",
        "\n",
        "# 정규화\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_flatten)\n",
        "X_test_scaled = scaler.transform(X_test_flatten)\n",
        "X_train = X_train_scaled.reshape(X_train.shape)\n",
        "X_test = X_test_scaled.reshape(X_test.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=X_train.shape[1:], kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(4, 4), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size=(4, 4), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(4, 4), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "print(\"Model Evaluate\")\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "# model.save('Hungry_Audio_model.h5')\n",
        "\n",
        "Run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", Run_time, \"sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5ZOolBglaKK",
        "outputId": "087b7ef4-46b1-471a-d55e-fd84ad60dc3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0, 1: 1}\n",
            "X_train shape: (335, 174, 188, 1)\n",
            "y_train shape: (335,)\n",
            "X_val shape: (84, 174, 188, 1)\n",
            "y_val shape: (84,)\n",
            "X_test shape: (30, 174, 188, 1)\n",
            "y_test shape: (30,)\n",
            "num_classes: 2\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.0388 - accuracy: 0.4925 - val_loss: 0.9309 - val_accuracy: 0.5119\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 62s 6s/step - loss: 0.8848 - accuracy: 0.4955 - val_loss: 0.8297 - val_accuracy: 0.5119\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 56s 5s/step - loss: 0.8010 - accuracy: 0.4866 - val_loss: 0.7660 - val_accuracy: 0.5119\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 56s 5s/step - loss: 0.7484 - accuracy: 0.4776 - val_loss: 0.7271 - val_accuracy: 0.5119\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 53s 5s/step - loss: 0.7188 - accuracy: 0.4836 - val_loss: 0.7071 - val_accuracy: 0.5119\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 65s 6s/step - loss: 0.7031 - accuracy: 0.5075 - val_loss: 0.6983 - val_accuracy: 0.5119\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 74s 6s/step - loss: 0.6979 - accuracy: 0.5075 - val_loss: 0.6954 - val_accuracy: 0.5119\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 57s 5s/step - loss: 0.6953 - accuracy: 0.5075 - val_loss: 0.6941 - val_accuracy: 0.5119\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 55s 5s/step - loss: 0.6946 - accuracy: 0.4776 - val_loss: 0.6935 - val_accuracy: 0.5119\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 53s 5s/step - loss: 0.6938 - accuracy: 0.5104 - val_loss: 0.6933 - val_accuracy: 0.5119\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 55s 5s/step - loss: 0.6936 - accuracy: 0.4716 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 53s 5s/step - loss: 0.6935 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 53s 5s/step - loss: 0.6934 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 54s 5s/step - loss: 0.6935 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 52s 5s/step - loss: 0.6934 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 54s 5s/step - loss: 0.6934 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 52s 5s/step - loss: 0.6934 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 56s 5s/step - loss: 0.6934 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 64s 6s/step - loss: 0.6934 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 55s 5s/step - loss: 0.6932 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5119\n",
            "Model Evaluate\n",
            "1/1 [==============================] - 1s 961ms/step - loss: 0.6875 - accuracy: 0.6667\n",
            "Test Loss: 0.6874929070472717\n",
            "Test Accuracy: 66.66666865348816 %\n",
            "Run_time : 1178.72 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Reshape, Conv2D, Flatten, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "# 기본 cuda GPU 사용, GPU 사용이 불가할 경우 CPU사용\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# CSV 파일과 음성 파일 경로를 이용하여 데이터 전처리 함수 정의\n",
        "def process_data(wav_path, csv_path):\n",
        "    X_mfcc = []  # MFCC 데이터를 저장할 리스트\n",
        "    X_spec = []  # Spectrogram 데이터를 저장할 리스트\n",
        "    labels = []  # 레이블을 저장할 리스트\n",
        "    data_dir = pathlib.Path(wav_path)\n",
        "    all_wav_paths = sorted(list(data_dir.glob('*.wav')))  # 정렬된 파일 경로 리스트\n",
        "\n",
        "    # CSV 파일 읽기\n",
        "    df = pd.read_csv(csv_path)\n",
        "    cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "    label = df[\"Label\"]\n",
        "\n",
        "    max_length = 188  # 패딩할 최대 길이\n",
        "\n",
        "    for wav_path_dir in all_wav_paths:\n",
        "        file_name = os.path.basename(wav_path_dir)\n",
        "        # 파일명과 일치하는 레이블 가져오기\n",
        "        index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "        label_value = label[index]\n",
        "\n",
        "        # 음성 파일 로드\n",
        "        y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "        # MFCC 추출\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        # MFCC 전처리\n",
        "        if mfcc.shape[1] > max_length:\n",
        "            mfcc = mfcc[:, :max_length]\n",
        "\n",
        "        # Spectrogram 추출\n",
        "        spec = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "        # Spectrogram 전처리\n",
        "        if spec.shape[1] > max_length:\n",
        "            spec = spec[:, :max_length]\n",
        "\n",
        "        X_mfcc.append(mfcc)\n",
        "        X_spec.append(spec)\n",
        "        labels.append(label_value)\n",
        "\n",
        "    X_mfcc = np.array(X_mfcc)\n",
        "    X_spec = np.array(X_spec)\n",
        "    labels = np.array(labels)\n",
        "    return X_mfcc, X_spec, labels\n",
        "\n",
        "# Train 데이터\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "# Test 데이터\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "# 데이터 전처리\n",
        "X_train_mfcc, X_train_spec, y_train = process_data(train_data_dir, train_csv_path)\n",
        "X_test_mfcc, X_test_spec, y_test = process_data(test_data_dir, test_csv_path)\n",
        "\n",
        "# Spectrogram 데이터에 채널 차원 추가\n",
        "X_train_spec = np.expand_dims(X_train_spec, axis=-1)\n",
        "X_test_spec = np.expand_dims(X_test_spec, axis=-1)\n",
        "\n",
        "# 데이터 스케일링(정규화)\n",
        "scaler_mfcc = StandardScaler()\n",
        "scaler_spec = StandardScaler()\n",
        "\n",
        "X_train_mfcc_scaled = scaler_mfcc.fit_transform(X_train_mfcc.reshape(-1, X_train_mfcc.shape[-1])).reshape(X_train_mfcc.shape)\n",
        "X_test_mfcc_scaled = scaler_mfcc.transform(X_test_mfcc.reshape(-1, X_test_mfcc.shape[-1])).reshape(X_test_mfcc.shape)\n",
        "X_train_spec_scaled = scaler_spec.fit_transform(X_train_spec.reshape(-1, X_train_spec.shape[-1])).reshape(X_train_spec.shape)\n",
        "X_test_spec_scaled = scaler_spec.transform(X_test_spec.reshape(-1, X_test_spec.shape[-1])).reshape(X_test_spec.shape)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# 원-핫 인코딩\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_train_encoded = to_categorical(y_train_encoded, num_classes=num_classes)\n",
        "y_test_encoded = to_categorical(y_test_encoded, num_classes=num_classes)\n",
        "\n",
        "# 데이터 분할\n",
        "X_train_mfcc_scaled, X_val_mfcc_scaled, X_train_spec_scaled, X_val_spec_scaled, y_train_encoded, y_val_encoded = train_test_split(X_train_mfcc_scaled, X_train_spec_scaled, y_train_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 생성\n",
        "mfcc_input = Input(shape=X_train_mfcc_scaled.shape[1:])\n",
        "mfcc_reshaped = Reshape((*X_train_mfcc_scaled.shape[1:], 1))(mfcc_input)\n",
        "mfcc_model = Conv2D(32, kernel_size=(4, 4), activation='relu')(mfcc_reshaped)\n",
        "mfcc_model = Conv2D(64, kernel_size=(3, 3), activation='relu')(mfcc_model)\n",
        "mfcc_model = Conv2D(128, kernel_size=(3, 3), activation='relu')(mfcc_model)\n",
        "\n",
        "spec_input = Input(shape=X_train_spec_scaled.shape[1:])\n",
        "spec_model = Conv2D(32, kernel_size=(4, 4), activation='relu')(spec_input)\n",
        "\n",
        "mfcc_model_flatten = Flatten()(mfcc_model)\n",
        "spec_model_flatten = Flatten()(spec_model)\n",
        "combined = concatenate([mfcc_model_flatten, spec_model_flatten])\n",
        "\n",
        "common = Dense(64, activation='relu')(combined)\n",
        "common = Dense(64, activation='relu')(common)\n",
        "common = Dropout(0.5)(common)\n",
        "output = Dense(num_classes, activation='softmax')(common)\n",
        "\n",
        "model = Model(inputs=[mfcc_input, spec_input], outputs=output)\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy']) # 학습률(Learning_rate -> 0.001로 조정)\n",
        "model.fit([X_train_mfcc_scaled, X_train_spec_scaled], y_train_encoded, batch_size=64, epochs=20, validation_data=([X_val_mfcc_scaled, X_val_spec_scaled], y_val_encoded))\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc_scaled, X_test_spec_scaled], y_test_encoded)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "\n",
        "run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", run_time, \"sec\")\n"
      ],
      "metadata": {
        "id": "4Y9iLDCVNlKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "할일\n",
        "1. 데이터 증강 -> 노이즈추가, 시간 이동, 속도 변환 고려.\n",
        "2. 모델 아키텍쳐 변경 -> 다른 CNN 또는 RNN 적용\n",
        "3. Regularization (규제) 사용. -> 과적합 방지 -> DropOut, 가중치 규제\n",
        "4. Optimizer 변경 -> Adam, SGD 등등\n",
        "5. 마지막 -> 전이 학습"
      ],
      "metadata": {
        "id": "cazevuI1oTnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "클래스화"
      ],
      "metadata": {
        "id": "sGPBFT0BuXFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Reshape, Conv2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Add\n",
        "from keras.optimizers import RMSprop\n",
        "# 기본 cuda GPU 사용, GPU 사용이 불가할 경우 CPU사용\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "import time\n",
        "start = time.time()\n",
        "# Class 구성\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from keras.utils import to_categorical\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from tensorflow import keras\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "\n",
        "        max_length = 188\n",
        "\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "\n",
        "            spec = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "            if spec.shape[1] > max_length:\n",
        "                spec = spec[:, :max_length]\n",
        "\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_spec.append(spec)\n",
        "            labels.append(label_value)\n",
        "\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_spec = np.array(X_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_spec, labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_spec, labels = self.process_data()\n",
        "\n",
        "        X_spec = np.expand_dims(X_spec, axis=-1)\n",
        "\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_spec = StandardScaler()\n",
        "\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_spec_scaled = scaler_spec.fit_transform(X_spec.reshape(-1, X_spec.shape[-1])).reshape(X_spec.shape)\n",
        "\n",
        "        # # 레이블 인코딩\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "        # 원-핫 인코딩\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        labels_encoded = to_categorical(labels_encoded, num_classes=num_classes)\n",
        "\n",
        "        return X_mfcc_scaled, X_spec_scaled, labels_encoded\n",
        "\n",
        "# Train 데이터\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "# Test 데이터\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "\n",
        "# 데이터 전처리\n",
        "train_classifier = AudioClassifier(train_data_dir, train_csv_path)\n",
        "X_train_mfcc, X_train_spec, y_train = train_classifier.preprocess_data()\n",
        "\n",
        "test_classifier = AudioClassifier(test_data_dir, test_csv_path)\n",
        "X_test_mfcc, X_test_spec, y_test = test_classifier.preprocess_data()\n",
        "\n",
        "# Spectrogram 데이터에 채널 차원 추가\n",
        "X_train_spec = np.expand_dims(X_train_spec, axis=-1)\n",
        "X_test_spec = np.expand_dims(X_test_spec, axis=-1)\n",
        "\n",
        "# 데이터 스케일링(정규화)\n",
        "scaler_mfcc = StandardScaler()\n",
        "scaler_spec = StandardScaler()\n",
        "\n",
        "X_train_mfcc_scaled = scaler_mfcc.fit_transform(X_train_mfcc.reshape(-1, X_train_mfcc.shape[-1])).reshape(X_train_mfcc.shape)\n",
        "X_test_mfcc_scaled = scaler_mfcc.transform(X_test_mfcc.reshape(-1, X_test_mfcc.shape[-1])).reshape(X_test_mfcc.shape)\n",
        "X_train_spec_scaled = scaler_spec.fit_transform(X_train_spec.reshape(-1, X_train_spec.shape[-1])).reshape(X_train_spec.shape)\n",
        "X_test_spec_scaled = scaler_spec.transform(X_test_spec.reshape(-1, X_test_spec.shape[-1])).reshape(X_test_spec.shape)\n",
        "\n",
        "# 데이터 형상 확인\n",
        "print(f\"X_train_mfcc_scaled : {X_train_mfcc_scaled.shape}\")\n",
        "print(f\"X_train_spec_scaled : {X_train_spec_scaled.shape}\")\n",
        "print(f\"X_test_mfcc_scaled : {X_test_mfcc_scaled.shape}\")\n",
        "print(f\"X_test_spec_scaled : {X_test_spec_scaled.shape}\")\n",
        "\n",
        "# print(f\"X_tarin_mfcc_scaled : {X_train_mfcc_scaled.shape}\")\n",
        "# print(f\"X_train_spec : {X_train_spec.shape}\")\n",
        "# print(f\"y_train : {y_train.shape}\")\n",
        "# print(f\"X_test_mfcc_scaled : {X_test_mfcc_scaled.shape}\")\n",
        "# print(f\"X_test_spec : {X_test_spec.shape}\")\n",
        "# print(f\"y_test : {y_test.shape}\")\n",
        "# 데이터 분할\n",
        "X_train_mfcc_scaled, X_val_mfcc_scaled, X_train_spec_scaled, X_val_spec_scaled, y_train_encoded, y_val_encoded = train_test_split(X_train_mfcc_scaled, X_train_spec_scaled, y_train, test_size=0.2, random_state=42)\n",
        "# Spectrogram 데이터에 채널 차원 변경\n",
        "X_train_spec_scaled = np.squeeze(X_train_spec_scaled, axis=-1)\n",
        "X_test_spec_scaled = np.squeeze(X_test_spec_scaled, axis=-1)\n",
        "# 수정된 데이터 형상 확인\n",
        "print(f\"X_train_spec_scaled : {X_train_spec_scaled.shape}\")\n",
        "print(f\"X_test_spec_scaled : {X_test_spec_scaled.shape}\")\n",
        "# 모델 생성\n",
        "mfcc_input = Input(shape=X_train_mfcc_scaled.shape[1:])\n",
        "mfcc_reshaped = Reshape((*X_train_mfcc_scaled.shape[1:], 1))(mfcc_input)\n",
        "mfcc_model = Conv2D(32, kernel_size=(4, 4), activation='relu')(mfcc_reshaped)\n",
        "mfcc_model = Conv2D(64, kernel_size=(3, 3), activation='relu')(mfcc_model)\n",
        "# mfcc_model = Conv2D(128, kernel_size=(3, 3), activation='relu')(mfcc_model)\n",
        "# mfcc_model = Conv2D(256, kernel_size=(3, 3), activation='relu')(mfcc_model)\n",
        "# mfcc_model = Conv2D(512, kernel_size=(3, 3), activation='relu')(mfcc_model)\n",
        "\n",
        "spec_input = Input(shape=X_train_spec_scaled.shape[1:])\n",
        "spec_model = Conv2D(32, kernel_size=(4, 4), activation='relu')(spec_input)\n",
        "spec_model = Conv2D(64, kernel_size=(3, 3), activation='relu')(spec_input)\n",
        "# spec_model = Conv2D(128, kernel_size=(3, 3), activation='relu')(spec_input)\n",
        "# spec_model = Conv2D(256, kernel_size=(3, 3), activation='relu')(spec_input)\n",
        "# spec_model = Conv2D(512, kernel_size=(3, 3), activation='relu')(spec_input)\n",
        "\n",
        "mfcc_model_flatten = Flatten()(mfcc_model)\n",
        "spec_model_flatten = Flatten()(spec_model)\n",
        "combined = concatenate([mfcc_model_flatten, spec_model_flatten])\n",
        "\n",
        "common = Dense(64, activation='relu')(combined)\n",
        "# common = Dropout(0.5)(combined)\n",
        "common = Dense(64, activation='relu')(common)\n",
        "output = Dense(2, activation='sigmoid')(common)\n",
        "\n",
        "model = Model(inputs=[mfcc_input, spec_input], outputs=output)\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy']) # 학습률(Learning_rate -> 0.001로 조정)\n",
        "model.fit([X_train_mfcc_scaled, X_train_spec_scaled], y_train_encoded, batch_size=16, epochs=20, validation_data=([X_val_mfcc_scaled, X_val_spec_scaled], y_val_encoded))\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc_scaled, X_test_spec_scaled], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "\n",
        "run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", run_time, \"sec\")\n"
      ],
      "metadata": {
        "id": "vW0iHXoGuF4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Block 사용"
      ],
      "metadata": {
        "id": "Y4SIbzdRbaIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, concatenate, Conv2D, Reshape, BatchNormalization, Activation, Add, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "\n",
        "        max_length = 188\n",
        "\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "\n",
        "            spec = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "            if spec.shape[1] > max_length:\n",
        "                spec = spec[:, :max_length]\n",
        "\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_spec.append(spec)\n",
        "            labels.append(label_value)\n",
        "\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_spec = np.array(X_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_spec, labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_spec, labels = self.process_data()\n",
        "\n",
        "        X_spec = np.expand_dims(X_spec, axis=-1)\n",
        "\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_spec = StandardScaler()\n",
        "\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_spec_scaled = scaler_spec.fit_transform(X_spec.reshape(-1, X_spec.shape[-1])).reshape(X_spec.shape)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        labels_encoded = to_categorical(labels_encoded, num_classes=num_classes)\n",
        "\n",
        "        return X_mfcc_scaled, X_spec_scaled, labels_encoded\n",
        "\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "train_classifier = AudioClassifier(train_data_dir, train_csv_path)\n",
        "X_train_mfcc, X_train_spec, y_train = train_classifier.preprocess_data()\n",
        "\n",
        "test_classifier = AudioClassifier(test_data_dir, test_csv_path)\n",
        "X_test_mfcc, X_test_spec, y_test = test_classifier.preprocess_data()\n",
        "# 데이터 분할 random_state=42\n",
        "X_train_mfcc_scaled, X_val_mfcc_scaled, X_train_spec_scaled, X_val_spec_scaled, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    X_train_mfcc, X_train_spec, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "# Spectrogram 데이터에 채널 차원 변경\n",
        "X_train_spec_scaled = np.squeeze(X_train_spec_scaled, axis=-1)\n",
        "X_val_spec_scaled = np.squeeze(X_val_spec_scaled, axis=-1)\n",
        "X_test_spec_scaled = np.squeeze(X_test_spec, axis=-1)\n",
        "\n",
        "# 수정된 데이터 형상 확인\n",
        "print(f\"X_train_spec_scaled : {X_train_spec_scaled.shape}\")\n",
        "print(f\"X_val_spec_scaled : {X_val_spec_scaled.shape}\")\n",
        "print(f\"X_test_spec_scaled : {X_test_spec_scaled.shape}\")\n",
        "# Residual Block 구현\n",
        "def residual_block(inputs, filters, kernel_size):\n",
        "    # x = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    # x = BatchNormalization()(x)\n",
        "    # x = Activation('relu')(x)\n",
        "    # x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    # x = Add()([inputs, x])\n",
        "    # x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([inputs, x])\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "# 모델 생성\n",
        "mfcc_input = Input(shape=X_train_mfcc_scaled.shape[1:])\n",
        "mfcc_reshaped = Reshape((*X_train_mfcc_scaled.shape[1:], 1))(mfcc_input)\n",
        "mfcc_model = Conv2D(32, kernel_size=(4, 4))(mfcc_reshaped)\n",
        "mfcc_model = BatchNormalization()(mfcc_model)\n",
        "mfcc_model = LeakyReLU()(mfcc_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "mfcc_model = residual_block(mfcc_model, 32, (4, 4))\n",
        "\n",
        "spec_input = Input(shape=(X_train_spec_scaled.shape[1], X_train_spec_scaled.shape[2], 1))\n",
        "spec_model = Conv2D(32, kernel_size=(4, 4))(spec_input)\n",
        "spec_model = BatchNormalization()(spec_model)\n",
        "spec_model = LeakyReLU()(spec_model)\n",
        "# spec_model = BatchNormalization()(spec_model)\n",
        "spec_model = LeakyReLU()(spec_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "spec_model = residual_block(spec_model, 32, (4, 4))\n",
        "\n",
        "mfcc_model_flatten = Flatten()(mfcc_model)\n",
        "spec_model_flatten = Flatten()(spec_model)\n",
        "combined = concatenate([mfcc_model_flatten, spec_model_flatten])\n",
        "\n",
        "common = Dense(64, activation='relu')(combined)\n",
        "# common = Dense(64, activation='relu')(common)\n",
        "output = Dense(2, activation='sigmoid')(common)\n",
        "\n",
        "model = Model(inputs=[mfcc_input, spec_input], outputs=output)\n",
        "# 모델 컴파일 및 학습 RMSprop(learning_rate=0.001)\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model.fit([X_train_mfcc_scaled, X_train_spec_scaled], y_train_encoded, batch_size=16, epochs=20, validation_data=([X_val_mfcc_scaled, X_val_spec_scaled], y_val_encoded))\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc, X_test_spec_scaled], y_test)\n",
        "\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "model.save('Audio_model_MFCC_Spectrogram.h5')\n",
        "\n",
        "run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", run_time, \"sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PowWXeeLhtNV",
        "outputId": "9e2d01fd-78ae-42af-b3c4-8ee3ae106b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_spec_scaled : (335, 1025, 188)\n",
            "X_val_spec_scaled : (84, 1025, 188)\n",
            "X_test_spec_scaled : (30, 1025, 188)\n",
            "Epoch 1/20\n",
            "21/21 [==============================] - 28s 775ms/step - loss: 220.3094 - accuracy: 0.5075 - val_loss: 11.3519 - val_accuracy: 0.5119\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 11s 528ms/step - loss: 35.7009 - accuracy: 0.6478 - val_loss: 19.7017 - val_accuracy: 0.4524\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 11s 535ms/step - loss: 15.1156 - accuracy: 0.7910 - val_loss: 15.6009 - val_accuracy: 0.4643\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 12s 554ms/step - loss: 5.1703 - accuracy: 0.8896 - val_loss: 14.6890 - val_accuracy: 0.5238\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 11s 551ms/step - loss: 2.1047 - accuracy: 0.9493 - val_loss: 27.6823 - val_accuracy: 0.4762\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 11s 522ms/step - loss: 3.5371 - accuracy: 0.9015 - val_loss: 10.3948 - val_accuracy: 0.4167\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 11s 521ms/step - loss: 0.6089 - accuracy: 0.9910 - val_loss: 9.7838 - val_accuracy: 0.4405\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 11s 522ms/step - loss: 0.1275 - accuracy: 0.9970 - val_loss: 16.3841 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 11s 546ms/step - loss: 0.0705 - accuracy: 1.0000 - val_loss: 10.7830 - val_accuracy: 0.4643\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 11s 524ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 10.1368 - val_accuracy: 0.4762\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 11s 550ms/step - loss: 0.3243 - accuracy: 0.9940 - val_loss: 15.5782 - val_accuracy: 0.4762\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 11s 550ms/step - loss: 0.1581 - accuracy: 0.9970 - val_loss: 27.3403 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 11s 524ms/step - loss: 0.0737 - accuracy: 0.9970 - val_loss: 12.0620 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 11s 536ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 16.1973 - val_accuracy: 0.4405\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 11s 548ms/step - loss: 5.1447e-04 - accuracy: 1.0000 - val_loss: 19.3265 - val_accuracy: 0.5238\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 11s 524ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 30.7749 - val_accuracy: 0.4762\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 11s 523ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 19.6431 - val_accuracy: 0.5119\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 11s 523ms/step - loss: 0.0313 - accuracy: 0.9970 - val_loss: 28.2919 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 11s 548ms/step - loss: 0.2018 - accuracy: 0.9940 - val_loss: 34.8430 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 11s 547ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 17.3985 - val_accuracy: 0.5238\n",
            "1/1 [==============================] - 3s 3s/step - loss: 6.9150 - accuracy: 0.7333\n",
            "Test Loss: 6.915025234222412\n",
            "Test Accuracy: 73.33333492279053 %\n",
            "Run_time : 349.18 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0은 정답 1은 오답으로 학습 하는 모델"
      ],
      "metadata": {
        "id": "A_QkkfpIbUuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, concatenate, Conv2D, Reshape, BatchNormalization, Activation, Add, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "\n",
        "        max_length = 188\n",
        "\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "\n",
        "            spec = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "            if spec.shape[1] > max_length:\n",
        "                spec = spec[:, :max_length]\n",
        "\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_spec.append(spec)\n",
        "            labels.append(label_value)\n",
        "\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_spec = np.array(X_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_spec, labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_spec, labels = self.process_data()\n",
        "\n",
        "        X_spec = np.expand_dims(X_spec, axis=-1)\n",
        "\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_spec = StandardScaler()\n",
        "\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_spec_scaled = scaler_spec.fit_transform(X_spec.reshape(-1, X_spec.shape[-1])).reshape(X_spec.shape)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "        return X_mfcc_scaled, X_spec_scaled, labels_encoded\n",
        "\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "train_classifier = AudioClassifier(train_data_dir, train_csv_path)\n",
        "X_train_mfcc, X_train_spec, y_train = train_classifier.preprocess_data()\n",
        "\n",
        "test_classifier = AudioClassifier(test_data_dir, test_csv_path)\n",
        "X_test_mfcc, X_test_spec, y_test = test_classifier.preprocess_data()\n",
        "# 데이터 분할 random_state=42\n",
        "X_train_mfcc_scaled, X_val_mfcc_scaled, X_train_spec_scaled, X_val_spec_scaled, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    X_train_mfcc, X_train_spec, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "# Spectrogram 데이터에 채널 차원 변경\n",
        "X_train_spec_scaled = np.squeeze(X_train_spec_scaled, axis=-1)\n",
        "X_val_spec_scaled = np.squeeze(X_val_spec_scaled, axis=-1)\n",
        "X_test_spec_scaled = np.squeeze(X_test_spec, axis=-1)\n",
        "\n",
        "# 수정된 데이터 형상 확인\n",
        "print(f\"X_train_spec_scaled : {X_train_spec_scaled.shape}\")\n",
        "print(f\"X_val_spec_scaled : {X_val_spec_scaled.shape}\")\n",
        "print(f\"X_test_spec_scaled : {X_test_spec_scaled.shape}\")\n",
        "# Residual Block 구현\n",
        "def residual_block(inputs, filters, kernel_size):\n",
        "    # x = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    # x = BatchNormalization()(x)\n",
        "    # x = Activation('relu')(x)\n",
        "    # x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    # x = Add()([inputs, x])\n",
        "    # x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([inputs, x])\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "# 모델 생성\n",
        "mfcc_input = Input(shape=X_train_mfcc_scaled.shape[1:])\n",
        "mfcc_reshaped = Reshape((*X_train_mfcc_scaled.shape[1:], 1))(mfcc_input)\n",
        "mfcc_model = Conv2D(32, kernel_size=(4, 4))(mfcc_reshaped)\n",
        "mfcc_model = BatchNormalization()(mfcc_model)\n",
        "mfcc_model = LeakyReLU()(mfcc_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "mfcc_model = residual_block(mfcc_model, 32, (4, 4))\n",
        "\n",
        "spec_input = Input(shape=(X_train_spec_scaled.shape[1], X_train_spec_scaled.shape[2], 1))\n",
        "spec_model = Conv2D(32, kernel_size=(4, 4))(spec_input)\n",
        "spec_model = BatchNormalization()(spec_model)\n",
        "spec_model = LeakyReLU()(spec_model)\n",
        "# spec_model = BatchNormalization()(spec_model)\n",
        "spec_model = LeakyReLU()(spec_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "spec_model = residual_block(spec_model, 32, (4, 4))\n",
        "\n",
        "mfcc_model_flatten = Flatten()(mfcc_model)\n",
        "spec_model_flatten = Flatten()(spec_model)\n",
        "combined = concatenate([mfcc_model_flatten, spec_model_flatten])\n",
        "\n",
        "common = Dense(64, activation='relu')(combined)\n",
        "# common = Dense(64, activation='relu')(common)\n",
        "output = Dense(1, activation='sigmoid')(common)\n",
        "\n",
        "model = Model(inputs=[mfcc_input, spec_input], outputs=output)\n",
        "# 모델 컴파일 및 학습 RMSprop(learning_rate=0.001)\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model.fit([X_train_mfcc_scaled, X_train_spec_scaled], y_train_encoded, batch_size=16, epochs=20, validation_data=([X_val_mfcc_scaled, X_val_spec_scaled], y_val_encoded))\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc, X_test_spec_scaled], y_test)\n",
        "\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "model.save('Audio_model_MFCC_Spectrogram.h5')\n",
        "\n",
        "run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", run_time, \"sec\")"
      ],
      "metadata": {
        "id": "qw5BltXJZfIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MFCC, Mel-Spectrogram을 이용한 모델"
      ],
      "metadata": {
        "id": "1KJoEQSEcC9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, concatenate, Conv2D, Reshape, BatchNormalization, Activation, Add, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "# from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "# hop_length\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_mel_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "\n",
        "        max_length = 188\n",
        "\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            mel_spec = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
        "            if mel_spec.shape[1] > max_length:\n",
        "                mel_spec = mel_spec[:, :max_length]\n",
        "\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_mel_spec.append(mel_spec)\n",
        "            labels.append(label_value)\n",
        "\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_mel_spec = np.array(X_mel_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_mel_spec, labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_mel_spec, labels = self.process_data()\n",
        "\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_mel_spec = StandardScaler()\n",
        "\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_mel_spec_scaled = scaler_mel_spec.fit_transform(X_mel_spec.reshape(-1, X_mel_spec.shape[-1])).reshape(X_mel_spec.shape)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "\n",
        "        return X_mfcc_scaled, X_mel_spec_scaled, labels_encoded, label_encoder, num_classes\n",
        "\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/New_train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "train_classifier = AudioClassifier(train_data_dir, train_csv_path)\n",
        "X_train_mfcc, X_train_mel_spec, y_train, label_encoder, num_classes = train_classifier.preprocess_data()\n",
        "\n",
        "test_classifier = AudioClassifier(test_data_dir, test_csv_path)\n",
        "X_test_mfcc, X_test_mel_spec, y_test, _, _ = test_classifier.preprocess_data()\n",
        "\n",
        "# 데이터 분할 random_state=42\n",
        "X_train_mfcc_scaled, X_val_mfcc_scaled, X_train_mel_spec_scaled, X_val_mel_spec_scaled, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    X_train_mfcc, X_train_mel_spec, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "print(f'X_train_mfcc_scaled : {X_train_mfcc_scaled.shape}')\n",
        "print(f'X_val_mfcc_scaled : {X_val_mfcc_scaled.shape}')\n",
        "print(f'X_train_mel_spec_scaled : {X_train_mel_spec_scaled.shape}')\n",
        "print(f'X_val_mel_spec_scaled : {X_val_mel_spec_scaled.shape}')\n",
        "print(f'y_train_encoded : {y_train_encoded.shape}')\n",
        "print(f'y_val_encoded : {y_val_encoded.shape}')\n",
        "print(f'num_classes : {num_classes}')\n",
        "\n",
        "def residual_block(inputs, filters, kernel_size):\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([inputs, x])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# 모델 생성\n",
        "mfcc_input = Input(shape=X_train_mfcc_scaled.shape[1:])\n",
        "mfcc_reshaped = Reshape((*X_train_mfcc_scaled.shape[1:], 1))(mfcc_input)\n",
        "mfcc_model = Conv2D(32, kernel_size=(4, 4))(mfcc_reshaped)\n",
        "mfcc_model = BatchNormalization()(mfcc_model)\n",
        "mfcc_model = LeakyReLU()(mfcc_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "mfcc_model = residual_block(mfcc_model, 32, (4, 4))\n",
        "mfcc_model = residual_block(mfcc_model, 32, (4, 4))\n",
        "\n",
        "mel_spec_input = Input(shape=(X_train_mel_spec_scaled.shape[1], X_train_mel_spec_scaled.shape[2], 1))\n",
        "mel_spec_model = Conv2D(32, kernel_size=(4, 4))(mel_spec_input)\n",
        "mel_spec_model = BatchNormalization()(mel_spec_model)\n",
        "mel_spec_model = LeakyReLU()(mel_spec_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "mel_spec_model = residual_block(mel_spec_model, 32, (4, 4))\n",
        "mel_spec_model = residual_block(mel_spec_model, 32, (4, 4))\n",
        "\n",
        "mfcc_model_flatten = Flatten()(mfcc_model)\n",
        "mel_spec_model_flatten = Flatten()(mel_spec_model)\n",
        "combined = concatenate([mfcc_model_flatten, mel_spec_model_flatten])\n",
        "\n",
        "common = Dense(64, activation='relu')(combined)\n",
        "# output = Dense(1, activation='sigmoid')(common)\n",
        "output = Dense(num_classes, activation='softmax')(common)\n",
        "\n",
        "learning_rate = 0.00006  # 학습률 값\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model = Model(inputs=[mfcc_input, mel_spec_input], outputs=output)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model.fit([X_train_mfcc_scaled, X_train_mel_spec_scaled], y_train_encoded, batch_size=16, shuffle=True, epochs=10, validation_data=([X_val_mfcc_scaled, X_val_mel_spec_scaled], y_val_encoded))\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc, X_test_mel_spec], y_test)\n",
        "model.save('Audio_Classify_Model.h5')\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "\n",
        "run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", run_time, \"sec\")"
      ],
      "metadata": {
        "id": "z5LVmoGhcCHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8328ab6-8e24-4f3a-94f0-c5c97e6f6cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_mfcc_scaled : (334, 13, 188)\n",
            "X_val_mfcc_scaled : (84, 13, 188)\n",
            "X_train_mel_spec_scaled : (334, 128, 188)\n",
            "X_val_mel_spec_scaled : (84, 128, 188)\n",
            "y_train_encoded : (334,)\n",
            "y_val_encoded : (84,)\n",
            "num_classes : 2\n",
            "Epoch 1/10\n",
            "21/21 [==============================] - 10s 135ms/step - loss: 92.7187 - accuracy: 0.6527 - val_loss: 13.5013 - val_accuracy: 0.7619\n",
            "Epoch 2/10\n",
            "21/21 [==============================] - 2s 118ms/step - loss: 0.8619 - accuracy: 0.9820 - val_loss: 6.5922 - val_accuracy: 0.7976\n",
            "Epoch 3/10\n",
            "21/21 [==============================] - 2s 120ms/step - loss: 0.2519 - accuracy: 0.9850 - val_loss: 1.3970 - val_accuracy: 0.9167\n",
            "Epoch 4/10\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.1050 - accuracy: 0.9940 - val_loss: 1.5158 - val_accuracy: 0.9405\n",
            "Epoch 5/10\n",
            "21/21 [==============================] - 2s 115ms/step - loss: 0.1493 - accuracy: 0.9940 - val_loss: 1.1131 - val_accuracy: 0.9524\n",
            "Epoch 6/10\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.5565 - val_accuracy: 0.8690\n",
            "Epoch 7/10\n",
            "21/21 [==============================] - 2s 115ms/step - loss: 0.0928 - accuracy: 0.9970 - val_loss: 1.5853 - val_accuracy: 0.9286\n",
            "Epoch 8/10\n",
            "21/21 [==============================] - 2s 112ms/step - loss: 0.2544 - accuracy: 0.9940 - val_loss: 0.8459 - val_accuracy: 0.9643\n",
            "Epoch 9/10\n",
            "21/21 [==============================] - 2s 110ms/step - loss: 0.7464 - accuracy: 0.9880 - val_loss: 1.2294 - val_accuracy: 0.9405\n",
            "Epoch 10/10\n",
            "21/21 [==============================] - 2s 116ms/step - loss: 0.4978 - accuracy: 0.9880 - val_loss: 0.7561 - val_accuracy: 0.9643\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.5553 - accuracy: 0.8857\n",
            "Test Loss: 1.5553473234176636\n",
            "Test Accuracy: 88.57142925262451 %\n",
            "Run_time : 67.36 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, concatenate, Conv2D, Reshape, BatchNormalization, Activation, Add, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "# from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "# hop_length\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_mel_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "\n",
        "        max_length = 188\n",
        "\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            mel_spec = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
        "            if mel_spec.shape[1] > max_length:\n",
        "                mel_spec = mel_spec[:, :max_length]\n",
        "\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_mel_spec.append(mel_spec)\n",
        "            labels.append(label_value)\n",
        "\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_mel_spec = np.array(X_mel_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_mel_spec, labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_mel_spec, labels = self.process_data()\n",
        "\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_mel_spec = StandardScaler()\n",
        "\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_mel_spec_scaled = scaler_mel_spec.fit_transform(X_mel_spec.reshape(-1, X_mel_spec.shape[-1])).reshape(X_mel_spec.shape)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "\n",
        "        return X_mfcc_scaled, X_mel_spec_scaled, labels_encoded, label_encoder, num_classes\n",
        "\n",
        "train_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/New_train_Audio.csv\"\n",
        "train_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data\"\n",
        "\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "train_classifier = AudioClassifier(train_data_dir, train_csv_path)\n",
        "X_train_mfcc, X_train_mel_spec, y_train, label_encoder, num_classes = train_classifier.preprocess_data()\n",
        "\n",
        "test_classifier = AudioClassifier(test_data_dir, test_csv_path)\n",
        "X_test_mfcc, X_test_mel_spec, y_test, _, _ = test_classifier.preprocess_data()\n",
        "\n",
        "# 데이터 분할 random_state=42\n",
        "X_train_mfcc_scaled, X_val_mfcc_scaled, X_train_mel_spec_scaled, X_val_mel_spec_scaled, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    X_train_mfcc, X_train_mel_spec, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "print(f'X_train_mfcc_scaled : {X_train_mfcc_scaled.shape}')\n",
        "print(f'X_val_mfcc_scaled : {X_val_mfcc_scaled.shape}')\n",
        "print(f'X_train_mel_spec_scaled : {X_train_mel_spec_scaled.shape}')\n",
        "print(f'X_val_mel_spec_scaled : {X_val_mel_spec_scaled.shape}')\n",
        "print(f'y_train_encoded : {y_train_encoded.shape}')\n",
        "print(f'y_val_encoded : {y_val_encoded.shape}')\n",
        "print(f'num_classes : {num_classes}')\n",
        "\n",
        "def residual_block(inputs, filters, kernel_size):\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([inputs, x])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# 모델 생성\n",
        "mfcc_input = Input(shape=X_train_mfcc_scaled.shape[1:])\n",
        "mfcc_reshaped = Reshape((*X_train_mfcc_scaled.shape[1:], 1))(mfcc_input)\n",
        "mfcc_model = Conv2D(32, kernel_size=(4, 4))(mfcc_reshaped)\n",
        "mfcc_model = BatchNormalization()(mfcc_model)\n",
        "mfcc_model = LeakyReLU()(mfcc_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "mfcc_model = residual_block(mfcc_model, 32, (4, 4))\n",
        "mfcc_model = residual_block(mfcc_model, 32, (4, 4))\n",
        "\n",
        "mel_spec_input = Input(shape=(X_train_mel_spec_scaled.shape[1], X_train_mel_spec_scaled.shape[2], 1))\n",
        "mel_spec_model = Conv2D(32, kernel_size=(4, 4))(mel_spec_input)\n",
        "mel_spec_model = BatchNormalization()(mel_spec_model)\n",
        "mel_spec_model = LeakyReLU()(mel_spec_model)\n",
        "\n",
        "# Residual Block 추가\n",
        "mel_spec_model = residual_block(mel_spec_model, 32, (4, 4))\n",
        "mel_spec_model = residual_block(mel_spec_model, 32, (4, 4))\n",
        "\n",
        "mfcc_model_flatten = Flatten()(mfcc_model)\n",
        "mel_spec_model_flatten = Flatten()(mel_spec_model)\n",
        "combined = concatenate([mfcc_model_flatten, mel_spec_model_flatten])\n",
        "\n",
        "common = Dense(64, activation='relu')(combined)\n",
        "# output = Dense(1, activation='sigmoid')(common)\n",
        "output = Dense(num_classes, activation='softmax')(common)\n",
        "\n",
        "learning_rate = 0.00006  # 학습률 값\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model = Model(inputs=[mfcc_input, mel_spec_input], outputs=output)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model.fit([X_train_mfcc_scaled, X_train_mel_spec_scaled], y_train_encoded, batch_size=16, shuffle=True, epochs=10, validation_data=([X_val_mfcc_scaled, X_val_mel_spec_scaled], y_val_encoded))\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc, X_test_mel_spec], y_test)\n",
        "model.save('Audio_Classify_Model.h5')\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "\n",
        "run_time = round(time.time() - start, 2)\n",
        "print(\"Run_time :\", run_time, \"sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZLQkgFvRWDm",
        "outputId": "55630cf8-5cd3-411f-b2b6-80c8384d624d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_mfcc_scaled : (334, 13, 188)\n",
            "X_val_mfcc_scaled : (84, 13, 188)\n",
            "X_train_mel_spec_scaled : (334, 128, 188)\n",
            "X_val_mel_spec_scaled : (84, 128, 188)\n",
            "y_train_encoded : (334,)\n",
            "y_val_encoded : (84,)\n",
            "num_classes : 2\n",
            "Epoch 1/10\n",
            "21/21 [==============================] - 24s 199ms/step - loss: 54.4903 - accuracy: 0.7635 - val_loss: 0.7174 - val_accuracy: 0.9762\n",
            "Epoch 2/10\n",
            "21/21 [==============================] - 2s 109ms/step - loss: 1.0360 - accuracy: 0.9880 - val_loss: 2.0578e-07 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "21/21 [==============================] - 2s 118ms/step - loss: 1.4674 - accuracy: 0.9850 - val_loss: 4.7660 - val_accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "21/21 [==============================] - 2s 118ms/step - loss: 1.9323 - accuracy: 0.9760 - val_loss: 5.7710 - val_accuracy: 0.8214\n",
            "Epoch 5/10\n",
            "21/21 [==============================] - 2s 108ms/step - loss: 0.4201 - accuracy: 0.9910 - val_loss: 0.5584 - val_accuracy: 0.9881\n",
            "Epoch 6/10\n",
            "21/21 [==============================] - 2s 116ms/step - loss: 1.9513 - accuracy: 0.9820 - val_loss: 0.7638 - val_accuracy: 0.9524\n",
            "Epoch 7/10\n",
            "21/21 [==============================] - 2s 117ms/step - loss: 1.1556 - accuracy: 0.9850 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "21/21 [==============================] - 2s 109ms/step - loss: 0.4667 - accuracy: 0.9970 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "21/21 [==============================] - 2s 118ms/step - loss: 0.1063 - accuracy: 0.9970 - val_loss: 9.7077e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "21/21 [==============================] - 2s 113ms/step - loss: 0.0869 - accuracy: 0.9970 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "2/2 [==============================] - 1s 158ms/step - loss: 2.8083 - accuracy: 0.9714\n",
            "Test Loss: 2.8083226680755615\n",
            "Test Accuracy: 97.14285731315613 %\n",
            "Run_time : 109.26 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 얼마나 정확히 맞추고있는지."
      ],
      "metadata": {
        "id": "tUhmg6BauHfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_mel_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "\n",
        "        max_length = 188\n",
        "\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            mel_spec = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
        "            if mel_spec.shape[1] > max_length:\n",
        "                mel_spec = mel_spec[:, :max_length]\n",
        "\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_mel_spec.append(mel_spec)\n",
        "            labels.append(label_value)\n",
        "\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_mel_spec = np.array(X_mel_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_mel_spec, labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_mel_spec, labels = self.process_data()\n",
        "\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_mel_spec = StandardScaler()\n",
        "\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_mel_spec_scaled = scaler_mel_spec.fit_transform(X_mel_spec.reshape(-1, X_mel_spec.shape[-1])).reshape(X_mel_spec.shape)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "\n",
        "        return X_mfcc_scaled, X_mel_spec_scaled, labels_encoded, label_encoder, num_classes\n",
        "\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test/test_Audio_New.csv\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "\n",
        "test_classifier = AudioClassifier(test_data_dir, test_csv_path)\n",
        "X_test_mfcc, X_test_mel_spec, y_test, _, _ = test_classifier.preprocess_data()\n",
        "\n",
        "# 모델 불러오기\n",
        "model = load_model('/content/drive/MyDrive/Audio_Classify_Model_0.2_97.h5')\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate([X_test_mfcc, X_test_mel_spec], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n"
      ],
      "metadata": {
        "id": "TURpQ7mCYrK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11ae783-5272-46df-88f4-90800e5ea12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2a4db07400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2583 - accuracy: 0.9714\n",
            "Test Loss: 0.258345365524292\n",
            "Test Accuracy: 97.14285731315613 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, wav_path, csv_path):\n",
        "        self.wav_path = wav_path\n",
        "        self.csv_path = csv_path\n",
        "    def process_data(self):\n",
        "        X_mfcc = []\n",
        "        X_mel_spec = []\n",
        "        labels = []\n",
        "        data_dir = pathlib.Path(self.wav_path)\n",
        "        all_wav_paths = sorted(list(data_dir.glob('*.wav')))\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        cry_audio_file = df[\"Cry_Audio_File\"]\n",
        "        label = df[\"Label\"]\n",
        "        max_length = 188\n",
        "        for wav_path_dir in all_wav_paths:\n",
        "            file_name = os.path.basename(wav_path_dir)\n",
        "            index = cry_audio_file[cry_audio_file == file_name].index[0]\n",
        "            label_value = label[index]\n",
        "            y, sr = librosa.load(wav_path_dir, sr=16000, duration=6)\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            if mfcc.shape[1] > max_length:\n",
        "                mfcc = mfcc[:, :max_length]\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            mel_spec = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
        "            if mel_spec.shape[1] > max_length:\n",
        "                mel_spec = mel_spec[:, :max_length]\n",
        "            X_mfcc.append(mfcc)\n",
        "            X_mel_spec.append(mel_spec)\n",
        "            labels.append(label_value)\n",
        "        X_mfcc = np.array(X_mfcc)\n",
        "        X_mel_spec = np.array(X_mel_spec)\n",
        "        labels = np.array(labels)\n",
        "        return X_mfcc, X_mel_spec, labels\n",
        "    def preprocess_data(self):\n",
        "        X_mfcc, X_mel_spec, labels = self.process_data()\n",
        "        scaler_mfcc = StandardScaler()\n",
        "        scaler_mel_spec = StandardScaler()\n",
        "        X_mfcc_scaled = scaler_mfcc.fit_transform(X_mfcc.reshape(-1, X_mfcc.shape[-1])).reshape(X_mfcc.shape)\n",
        "        X_mel_spec_scaled = scaler_mel_spec.fit_transform(X_mel_spec.reshape(-1, X_mel_spec.shape[-1])).reshape(X_mel_spec.shape)\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels_encoded = label_encoder.fit_transform(labels)\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        return X_mfcc_scaled, X_mel_spec_scaled, labels_encoded, label_encoder, num_classes\n",
        "\n",
        "test_data_dir = \"/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test\"\n",
        "test_classifier = AudioClassifier(test_data_dir, _)\n",
        "X_test_mfcc, X_test_mel_spec, y_test, _, _ = test_classifier.preprocess_data()\n",
        "\n",
        "model = tf.keras.models.load_model('/content/Hungry_Audio_model.h5')\n",
        "print(\"Model Evaluate\")\n",
        "predictions = model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# 정답 데이터 로드\n",
        "test_csv_path = \"/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv\"\n",
        "df = pd.read_csv(test_csv_path)\n",
        "true_labels = df[\"Label\"].to_numpy()\n",
        "\n",
        "# 예측 결과 출력\n",
        "print(\"Predictions:\")\n",
        "correct_count = 0\n",
        "total_count = len(true_labels)\n",
        "for i in range(len(predicted_labels)):\n",
        "    prediction = predicted_labels[i]\n",
        "    true_label = true_labels[i]\n",
        "    is_correct = prediction == true_label\n",
        "    if is_correct:\n",
        "        correct_count += 1\n",
        "    print(f\"Sample {i+1}: 예상={prediction}, 정답={true_label} (예측={is_correct})\")\n",
        "\n",
        "accuracy = correct_count / total_count\n",
        "print(f\"\\nAccuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU1c8ZEMVifi",
        "outputId": "733d3ef8-46c4-42cf-cc1e-101584d69194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluate\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Predictions:\n",
            "Sample 1: 예상=0, 정답=0 (예측=True)\n",
            "Sample 2: 예상=0, 정답=0 (예측=True)\n",
            "Sample 3: 예상=0, 정답=0 (예측=True)\n",
            "Sample 4: 예상=0, 정답=0 (예측=True)\n",
            "Sample 5: 예상=0, 정답=0 (예측=True)\n",
            "Sample 6: 예상=0, 정답=0 (예측=True)\n",
            "Sample 7: 예상=0, 정답=0 (예측=True)\n",
            "Sample 8: 예상=0, 정답=0 (예측=True)\n",
            "Sample 9: 예상=0, 정답=0 (예측=True)\n",
            "Sample 10: 예상=0, 정답=0 (예측=True)\n",
            "Sample 11: 예상=0, 정답=0 (예측=True)\n",
            "Sample 12: 예상=0, 정답=0 (예측=True)\n",
            "Sample 13: 예상=0, 정답=0 (예측=True)\n",
            "Sample 14: 예상=0, 정답=0 (예측=True)\n",
            "Sample 15: 예상=0, 정답=0 (예측=True)\n",
            "Sample 16: 예상=0, 정답=0 (예측=True)\n",
            "Sample 17: 예상=0, 정답=0 (예측=True)\n",
            "Sample 18: 예상=0, 정답=0 (예측=True)\n",
            "Sample 19: 예상=0, 정답=0 (예측=True)\n",
            "Sample 20: 예상=0, 정답=0 (예측=True)\n",
            "Sample 21: 예상=0, 정답=1 (예측=False)\n",
            "Sample 22: 예상=0, 정답=1 (예측=False)\n",
            "Sample 23: 예상=0, 정답=1 (예측=False)\n",
            "Sample 24: 예상=0, 정답=1 (예측=False)\n",
            "Sample 25: 예상=0, 정답=1 (예측=False)\n",
            "Sample 26: 예상=0, 정답=1 (예측=False)\n",
            "Sample 27: 예상=0, 정답=1 (예측=False)\n",
            "Sample 28: 예상=0, 정답=1 (예측=False)\n",
            "Sample 29: 예상=0, 정답=1 (예측=False)\n",
            "Sample 30: 예상=0, 정답=1 (예측=False)\n",
            "\n",
            "Accuracy: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7-2 모델 저장 및 JSON 형식으로 변환"
      ],
      "metadata": {
        "id": "Vzoz8eFFfTtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "# TensorFlow 모델을 로드합니다.\n",
        "model = tf.keras.models.load_model('/content/Hungry_Audio_model.h5')\n",
        "\n",
        "# 모델을 JSON 파일로 변환합니다.\n",
        "save_path = '/content/drive/MyDrive/Hungry_Audio_model.json'\n",
        "tfjs.converters.save_keras_model(model, save_path)"
      ],
      "metadata": {
        "id": "8jJF6YnoaJI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrgkYDMzNu49"
      },
      "source": [
        "8-1. Train Audio csv 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLuLG807B7rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c84615-b1d9-4369-c4bd-3e2153f22b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-454716d4e574>:22: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead.\n",
            "  stft_mean = librosa.amplitude_to_db(librosa.stft(y=data)).mean()\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "# Define the path to the directory containing audio files\n",
        "audio_dir = '/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data'\n",
        "\n",
        "# Create an empty list to store the feature data for each audio file\n",
        "feature_data = []\n",
        "\n",
        "# Loop over each file in the directory\n",
        "for file_name in os.listdir(audio_dir):\n",
        "    if file_name.endswith('.wav'):\n",
        "        # Load the audio file\n",
        "        file_path = os.path.join(audio_dir, file_name)\n",
        "        data, sample_rate = librosa.load(file_path, sr=16000)\n",
        "\n",
        "        # Extract the features\n",
        "        amplitude_envelope_mean = data.mean()\n",
        "        rms_mean = librosa.feature.rms(y=data)[0].mean()\n",
        "        zcr_mean = librosa.feature.zero_crossing_rate(y=data)[0].mean()\n",
        "        stft_mean = librosa.amplitude_to_db(librosa.stft(y=data)).mean()\n",
        "        sc_mean = librosa.feature.spectral_contrast(y=data, sr=sample_rate).mean()\n",
        "        sban_mean = librosa.feature.spectral_bandwidth(y=data, sr=sample_rate).mean()\n",
        "        scon_mean = librosa.feature.spectral_contrast(y=data, sr=sample_rate).mean()\n",
        "        mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13)\n",
        "        mfccs13 = mfccs.mean(axis=1)\n",
        "        delmfccs13 = librosa.feature.delta(mfccs, order=1).mean(axis=1)\n",
        "        del2mfccs13 = librosa.feature.delta(mfccs, order=2).mean(axis=1)\n",
        "        melspec = librosa.feature.melspectrogram(y=data, sr=sample_rate)\n",
        "        mfccs20 = librosa.feature.mfcc(S=librosa.power_to_db(melspec), n_mfcc=20).mean(axis=1)\n",
        "\n",
        "        # Append the feature data for this file to the list\n",
        "        feature_data.append({\n",
        "            'Cry_Audio_File': file_name,\n",
        "            'Label': 1,\n",
        "            # 'Amplitude_Envelope_Mean': amplitude_envelope_mean,\n",
        "            # 'RMS_Mean': rms_mean,\n",
        "            # 'ZCR_Mean': zcr_mean,\n",
        "            # 'STFT_Mean': stft_mean,\n",
        "            # 'SC_Mean': sc_mean,\n",
        "            # 'SBAN_Mean': sban_mean,\n",
        "            # 'SCON_Mean': scon_mean,\n",
        "            # 'MFCCs13': mfccs13,\n",
        "            # 'delMFCCs13': delmfccs13,\n",
        "            # 'del2MFCCs13': del2mfccs13,\n",
        "            # 'MelSpec': melspec.mean(axis=1),\n",
        "            # 'MFCCs20': mfccs20,\n",
        "            # 'MFCCs1': mfccs[0],\n",
        "            # 'MFCCs2': mfccs[1],\n",
        "            # 'MFCCs3': mfccs[2],\n",
        "            # 'MFCCs4': mfccs[3],\n",
        "            # 'MFCCs5': mfccs[4],\n",
        "            # 'MFCCs6': mfccs[5],\n",
        "            # 'MFCCs7': mfccs[6],\n",
        "            # 'MFCCs8': mfccs[7],\n",
        "            # 'MFCCs9': mfccs[8],\n",
        "            # 'MFCCs10': mfccs[9],\n",
        "            # 'MFCCs11': mfccs[10],\n",
        "            # 'MFCCs12': mfccs[11],\n",
        "            # 'MFCCS13': mfccs[12]\n",
        "        })\n",
        "feature_data = sorted(feature_data, key=lambda x: x['Cry_Audio_File'])\n",
        "df = pd.DataFrame(feature_data)\n",
        "df.to_csv('/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-1 Train Audio csv 파일 확인"
      ],
      "metadata": {
        "id": "uHphLbu20t26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "f = open('/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data/train_Audio.csv','r')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "for line in rdr:\n",
        "    print(line)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "uPrxOsocz_3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUjxs0IRN4wv"
      },
      "source": [
        "8-2. Test Audio csv 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seoAKj2ANkYs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "# Define the path to the directory containing audio files\n",
        "audio_dir = '/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test'\n",
        "\n",
        "# Create an empty list to store the feature data for each audio file\n",
        "feature_data = []\n",
        "\n",
        "# Loop over each file in the directory\n",
        "for file_name in os.listdir(audio_dir):\n",
        "    if file_name.endswith('.wav'):\n",
        "        # Load the audio file\n",
        "        file_path = os.path.join(audio_dir, file_name)\n",
        "        data, sample_rate = librosa.load(file_path, sr=16000)\n",
        "\n",
        "        # Extract the features\n",
        "        # amplitude_envelope_mean = data.mean()\n",
        "        # rms_mean = librosa.feature.rms(y=data)[0].mean()\n",
        "        # zcr_mean = librosa.feature.zero_crossing_rate(y=data)[0].mean()\n",
        "        # stft_mean = librosa.amplitude_to_db(librosa.stft(y=data)).mean()\n",
        "        # sc_mean = librosa.feature.spectral_contrast(y=data, sr=sample_rate).mean()\n",
        "        # sban_mean = librosa.feature.spectral_bandwidth(y=data, sr=sample_rate).mean()\n",
        "        # scon_mean = librosa.feature.spectral_contrast(y=data, sr=sample_rate).mean()\n",
        "        # mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13)\n",
        "        # mfccs13 = mfccs.mean(axis=1)\n",
        "        # delmfccs13 = librosa.feature.delta(mfccs, order=1).mean(axis=1)\n",
        "        # del2mfccs13 = librosa.feature.delta(mfccs, order=2).mean(axis=1)\n",
        "        # melspec = librosa.feature.melspectrogram(y=data, sr=sample_rate)\n",
        "        # mfccs20 = librosa.feature.mfcc(S=librosa.power_to_db(melspec), n_mfcc=20).mean(axis=1)\n",
        "\n",
        "        # Append the feature data for this file to the list\n",
        "        feature_data.append({\n",
        "            'Cry_Audio_File': file_name,\n",
        "            'Label': 0,\n",
        "            # 'Amplitude_Envelope_Mean': amplitude_envelope_mean,\n",
        "            # 'RMS_Mean': rms_mean,\n",
        "            # 'ZCR_Mean': zcr_mean,\n",
        "            # 'STFT_Mean': stft_mean,\n",
        "            # 'SC_Mean': sc_mean,\n",
        "            # 'SBAN_Mean': sban_mean,\n",
        "            # 'SCON_Mean': scon_mean,\n",
        "            # 'MFCCs13': mfccs13,\n",
        "            # 'delMFCCs13': delmfccs13,\n",
        "            # 'del2MFCCs13': del2mfccs13,\n",
        "            # 'MelSpec': melspec.mean(axis=1),\n",
        "            # 'MFCCs20': mfccs20,\n",
        "            # 'MFCCs1': mfccs[0],\n",
        "            # 'MFCCs2': mfccs[1],\n",
        "            # 'MFCCs3': mfccs[2],\n",
        "            # 'MFCCs4': mfccs[3],\n",
        "            # 'MFCCs5': mfccs[4],\n",
        "            # 'MFCCs6': mfccs[5],\n",
        "            # 'MFCCs7': mfccs[6],\n",
        "            # 'MFCCs8': mfccs[7],\n",
        "            # 'MFCCs9': mfccs[8],\n",
        "            # 'MFCCs10': mfccs[9],\n",
        "            # 'MFCCs11': mfccs[10],\n",
        "            # 'MFCCs12': mfccs[11],\n",
        "            # 'MFCCS13': mfccs[12]\n",
        "        })\n",
        "\n",
        "feature_data = sorted(feature_data, key=lambda x: x['Cry_Audio_File'])\n",
        "df = pd.DataFrame(feature_data)\n",
        "df.to_csv('/content/drive/MyDrive/Baby_Sound/Hungry/test_Audio_New.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-2 Test Audio csv 파일 확인"
      ],
      "metadata": {
        "id": "xdZ-FUm702EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "f = open('/content/drive/MyDrive/Baby_Sound/Hungry/Classificant_Audio_data_test/test_Audio.csv','r')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "for line in rdr:\n",
        "    print(line)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "CtJLSMdB0PDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-3 pro_Audio 파일 csv파일로 저장"
      ],
      "metadata": {
        "id": "Xe6kiTmCK6Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "# Define the path to the directory containing audio files\n",
        "audio_dir = '/content/drive/MyDrive/pro_Audio'\n",
        "\n",
        "# Create an empty list to store the feature data for each audio file\n",
        "feature_data = []\n",
        "\n",
        "# Loop over each file in the directory\n",
        "for file_name in os.listdir(audio_dir):\n",
        "    if file_name.endswith('.wav'):\n",
        "        # Load the audio file\n",
        "        file_path = os.path.join(audio_dir, file_name)\n",
        "        data, sample_rate = librosa.load(file_path, sr=16000)\n",
        "\n",
        "        # Extract the features\n",
        "        amplitude_envelope_mean = data.mean()\n",
        "        rms_mean = librosa.feature.rms(y=data)[0].mean()\n",
        "        zcr_mean = librosa.feature.zero_crossing_rate(y=data)[0].mean()\n",
        "        stft_mean = librosa.amplitude_to_db(librosa.stft(y=data)).mean()\n",
        "        sc_mean = librosa.feature.spectral_contrast(y=data, sr=sample_rate).mean()\n",
        "        sban_mean = librosa.feature.spectral_bandwidth(y=data, sr=sample_rate).mean()\n",
        "        scon_mean = librosa.feature.spectral_contrast(y=data, sr=sample_rate).mean()\n",
        "        mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13)\n",
        "        mfccs13 = mfccs.mean(axis=1)\n",
        "        delmfccs13 = librosa.feature.delta(mfccs, order=1).mean(axis=1)\n",
        "        del2mfccs13 = librosa.feature.delta(mfccs, order=2).mean(axis=1)\n",
        "        melspec = librosa.feature.melspectrogram(y=data, sr=sample_rate)\n",
        "        mfccs20 = librosa.feature.mfcc(S=librosa.power_to_db(melspec), n_mfcc=20).mean(axis=1)\n",
        "\n",
        "        # Append the feature data for this file to the list\n",
        "        feature_data.append({\n",
        "            'Cry_Audio_File': file_name,\n",
        "            'Label': 1,\n",
        "            # 'Amplitude_Envelope_Mean': amplitude_envelope_mean,\n",
        "            # 'RMS_Mean': rms_mean,\n",
        "            # 'ZCR_Mean': zcr_mean,\n",
        "            # 'STFT_Mean': stft_mean,\n",
        "            # 'SC_Mean': sc_mean,\n",
        "            # 'SBAN_Mean': sban_mean,\n",
        "            # 'SCON_Mean': scon_mean,\n",
        "            # 'MFCCs13': mfccs13,\n",
        "            # 'delMFCCs13': delmfccs13,\n",
        "            # 'del2MFCCs13': del2mfccs13,\n",
        "            # 'MelSpec': melspec.mean(axis=1),\n",
        "            # 'MFCCs20': mfccs20,\n",
        "            # 'MFCCs1': mfccs[0],\n",
        "            # 'MFCCs2': mfccs[1],\n",
        "            # 'MFCCs3': mfccs[2],\n",
        "            # 'MFCCs4': mfccs[3],\n",
        "            # 'MFCCs5': mfccs[4],\n",
        "            # 'MFCCs6': mfccs[5],\n",
        "            # 'MFCCs7': mfccs[6],\n",
        "            # 'MFCCs8': mfccs[7],\n",
        "            # 'MFCCs9': mfccs[8],\n",
        "            # 'MFCCs10': mfccs[9],\n",
        "            # 'MFCCs11': mfccs[10],\n",
        "            # 'MFCCs12': mfccs[11],\n",
        "            # 'MFCCS13': mfccs[12]\n",
        "        })\n",
        "feature_data = sorted(feature_data, key=lambda x: x['Cry_Audio_File'])\n",
        "df = pd.DataFrame(feature_data)\n",
        "df.to_csv('/content/drive/MyDrive/Baby_Sound/Hungry/Baby_Crying_Audio_SCV/pro_Audio.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8PX7TGbgK4xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-3 pro_Audio csv 파일 확인"
      ],
      "metadata": {
        "id": "0Pj0dEGSLKDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "f = open('/content/drive/MyDrive/Baby_Sound/Hungry/Baby_Crying_Audio_SCV/pro_Audio.csv','r')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "for line in rdr:\n",
        "    print(line)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "QYZJmV2LLHZ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1DJH3ip3piHZVJ_zN6TN0CO1oR3wZIdQb",
      "authorship_tag": "ABX9TyMvSSBNtprp7HnBgMf34DPM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
